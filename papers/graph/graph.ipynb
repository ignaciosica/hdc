{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from graph import process_dataset, transform, centrality\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from hdc import hdv, bind, bundle, sbundle, ItemMemory, hdvw, hdva, cosim, hdvsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_graphe -> graphHD (graph, vertices, dimensions)\n",
    "\n",
    "\n",
    "def encode_graphe(graph, vertices, dimensions):\n",
    "    for node in graph.nodes:\n",
    "        if node not in vertices:\n",
    "            vertices[node] = hdv(dimensions)\n",
    "\n",
    "    Es = []\n",
    "\n",
    "    for edge in graph.edges:\n",
    "        v1 = vertices[edge[0]]\n",
    "        v2 = vertices[edge[1]]\n",
    "        Es.append(bind([v1, v2]))\n",
    "\n",
    "    return bundle(Es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_graphew -> vertices with hdw and edges (graph, vertices, base)\n",
    "\n",
    "\n",
    "def encode_graphew(graph, vertices, base):\n",
    "    for node in graph.nodes:\n",
    "        if node not in vertices:\n",
    "            vertices[node] = hdvw(base, float(node))\n",
    "\n",
    "    Es = []\n",
    "\n",
    "    for edge in graph.edges:\n",
    "        v1 = vertices[edge[0]]\n",
    "        v2 = vertices[edge[1]]\n",
    "        Es.append(bind([v1, v2]))\n",
    "\n",
    "    return bundle(Es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_graphv -> vertices with hdv and no edges (graph, vertices, base)\n",
    "\n",
    "\n",
    "def encode_graphv(graph, vertices, dimensions):\n",
    "    Vs = []\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        if node not in vertices:\n",
    "            vertices[node] = hdv(dimensions)\n",
    "        Vs.append(vertices[node])\n",
    "\n",
    "    return bundle(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_graphvw -> vertices with hdw and no edges (graph, vertices, base)\n",
    "\n",
    "\n",
    "def encode_graphvw(graph, vertices, base):\n",
    "    Vs = []\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        if node not in vertices:\n",
    "            vertices[node] = hdvw(base, float(node))\n",
    "        Vs.append(vertices[node])\n",
    "\n",
    "    return bundle(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_graphvc -> vertices with hdvc and no edges (graph, vertices, vs)\n",
    "\n",
    "\n",
    "def encode_graphvc(graph, vertices, vs):  # [0-1] encoded into hvds with precision n\n",
    "    return bundle(map(lambda n: vs[round(float(n) * len(vs))], graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_graphrw(graph, vertices, pm): # random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, encoder, transformer, step=20):\n",
    "        self.encoder = encoder\n",
    "        self.transformer = transformer\n",
    "        self.step = step\n",
    "        self.memory = ItemMemory()\n",
    "        self.vertices = dict()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes = {label: [] for label in set(y)}\n",
    "        X = self.transformer(X)\n",
    "        # X = transform(X, self.centrality, self.digits)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            G = self.encoder(X[i], self.vertices)\n",
    "            classes[y[i]].append(G)\n",
    "\n",
    "        for key, value in classes.items():\n",
    "            for i in range(0, len(value), self.step):\n",
    "                H = bundle(value[i : i + self.step])\n",
    "                self.memory.add_vector(str(key), H)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        p, s, queries = [], [], self.transformer(X)\n",
    "\n",
    "        for query in queries:\n",
    "            query_vector = self.encoder(query, self.vertices)\n",
    "            (label, _, _) = self.memory.cleanup(query_vector)\n",
    "\n",
    "            p.append(int(label))\n",
    "            # s.append(cosine_similarity(queryVector, cleanVector[1]))\n",
    "\n",
    "        # print(\"%.5f\" % round(np.mean(s), 5), \"0:\", p.count(0), \"1:\", p.count(1))\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"graphs-datasets/MUTAG\")[\"train\"]\n",
    "# DATASET = load_dataset(\"graphs-datasets/PROTEINS\")[\"train\"]\n",
    "# DATASET = load_dataset(\"graphs-datasets/AIDS\")[\"full\"]\n",
    "# DATASET = load_dataset(\"graphs-datasets/IMDB-BINARY\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graphs, labels) = process_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "\n",
    "FOLDS, REPS = 10, 3\n",
    "ALPHA, DIGITS, DIMENSIONS, STEP, N = 0.65, 4, 10000, 4, 1000\n",
    "CV = FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = hdvsc(N, DIMENSIONS, side=500, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank\n"
     ]
    }
   ],
   "source": [
    "encoders = {\n",
    "    \"encode_graphe\": partial(encode_graphe, dimensions=DIMENSIONS),\n",
    "    \"encode_graphew\": partial(encode_graphew, base=hdv(DIMENSIONS)),\n",
    "    \"encode_graphv\": partial(encode_graphv, dimensions=DIMENSIONS),\n",
    "    \"encode_graphvw\": partial(encode_graphvw, base=hdv(DIMENSIONS)),\n",
    "    # partial(encode_graphvc, vs=vs),\n",
    "}\n",
    "\n",
    "transformers = {\n",
    "    \"pagerank\": partial(centrality, rank=partial(nx.pagerank, alpha=ALPHA)),\n",
    "    \"eigenvector_centrality_numpy\": partial(\n",
    "        centrality, rank=partial(nx.eigenvector_centrality_numpy)\n",
    "    ),\n",
    "    \"katz_centrality_numpy\": partial(\n",
    "        centrality, rank=partial(nx.katz_centrality_numpy, alpha=ALPHA)\n",
    "    ),\n",
    "    \"degree_centrality\": partial(centrality, rank=partial(nx.degree_centrality)),\n",
    "    \"betweenness_centrality\": partial(\n",
    "        centrality, rank=partial(nx.betweenness_centrality)\n",
    "    ),\n",
    "    \"current_flow_closeness_centrality\": partial(\n",
    "        centrality, rank=partial(nx.current_flow_closeness_centrality)\n",
    "    ),\n",
    "    \"closeness_centrality\": partial(centrality, rank=partial(nx.closeness_centrality)),\n",
    "    \"edge_current_flow_betweenness_centrality\": partial(\n",
    "        centrality, rank=partial(nx.edge_current_flow_betweenness_centrality)\n",
    "    ),\n",
    "    \"laplacian_centrality\": partial(centrality, rank=partial(nx.laplacian_centrality)),\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    for tl, transformer in transformers.items():\n",
    "        print(tl)\n",
    "        esum = 0\n",
    "        for el, encoder in encoders.items():\n",
    "            clf = GraphClassifier(encoder, transformer, step=STEP)\n",
    "            sum = 0\n",
    "            start_time = time.time()\n",
    "            for i in range(REPS):\n",
    "                CV = ShuffleSplit()  # random_state=0\n",
    "                scores = cross_val_score(\n",
    "                    clf,\n",
    "                    graphs,\n",
    "                    labels,\n",
    "                    n_jobs=-1,\n",
    "                    cv=CV,\n",
    "                    verbose=0,\n",
    "                    error_score=\"raise\",\n",
    "                )\n",
    "                sum += scores.mean()\n",
    "                esum += scores.mean()\n",
    "            end_time = time.time()\n",
    "            print(\n",
    "                \"  Acc => %.5f\" % (sum / REPS),\n",
    "                \"T => %.5f\" % ((end_time - start_time) / REPS),\n",
    "                el,\n",
    "            )\n",
    "        print(\" Acc => %.5f\" % (esum / (REPS * len(encoders))))\n",
    "\n",
    "\n",
    "def predict():\n",
    "    for encoder in encoders:\n",
    "        clf = GraphClassifier(encoder, alpha=ALPHA, digits=DIGITS, step=STEP)\n",
    "        clf.fit(graphs[:150], labels[:150])\n",
    "        print(clf.predict(graphs[150:]))\n",
    "        print(labels[150:])\n",
    "\n",
    "\n",
    "main()\n",
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "def similarity_heatmap(xs, targets, round=2):\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=1.0)\n",
    "\n",
    "    plt.figure(figsize=(24, 2))\n",
    "    for n in range(len(targets)):\n",
    "        plt.subplot(len(targets) * 2, 1, n + 2)\n",
    "        sims = []\n",
    "\n",
    "        for x in xs:\n",
    "            sims.append(cosim(targets[n], x))\n",
    "\n",
    "        xaxis = [x + 0.5 for x in range(len(xs))]\n",
    "        X_Y_Spline = make_interp_spline(xaxis, sims)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.xlim(xmin=0, xmax=len(xs))\n",
    "        X_ = np.linspace(0, len(xs), len(xs) * 16 * 2)\n",
    "        Y_ = X_Y_Spline(X_)\n",
    "        plt.plot(X_, Y_, linewidth=1)\n",
    "\n",
    "        plt.subplot(len(targets) * 2, 1, n + 1)\n",
    "        plt.yticks([]), plt.xticks([])\n",
    "        plt.pcolormesh([sims] * 1, cmap=\"plasma\", norm=norm)\n",
    "        if round >= 0:\n",
    "            for i in range(len(xs)):\n",
    "                plt.text(\n",
    "                    i + 0.5,\n",
    "                    0 + 0.5,\n",
    "                    f\"%.{round}f\" % sims[i],\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    rotation=\"vertical\",\n",
    "                )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "similarity_heatmap(vs[:50], [vs[5]], round=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
